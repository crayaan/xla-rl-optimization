{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Below will print the HLO representation of an implementation of MHA self attention\n",
        "\n",
        "\n",
        "```\n",
        "Performs multi-head self-attention.\n",
        "Args:\n",
        "   params: Tuple containing the weight matrices (wq, wk, wv, wo).\n",
        "\n",
        "   x: Input tensor of shape (batch_size, sequence_length, embedding_dimension).\n",
        "\n",
        "   num_heads: Number of attention heads.\n",
        "\n",
        "   head_dim: Dimensionality of each attention head.\n",
        "   \n",
        "Returns:\n",
        "   Output tensor of shape (batch_size, sequence_length, embedding_dimension).\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "YB20Y8eoexLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.nn import softmax\n",
        "\n",
        "def multihead_self_attention(params, x, num_heads):\n",
        "  wq, wk, wv, wo = params # Query, Key, Value, Output weights\n",
        "  batch_size, seq_len, embed_dim = x.shape\n",
        "  head_dim = embed_dim // num_heads\n",
        "\n",
        "  # Project input to Q, K, V for each head\n",
        "  q = jnp.dot(x, wq).reshape(batch_size, num_heads, seq_len, head_dim)\n",
        "  k = jnp.dot(x, wk).reshape(batch_size, num_heads, seq_len, head_dim)\n",
        "  v = jnp.dot(x, wv).reshape(batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "  # Calculate attention scores for each head\n",
        "  scores = jnp.einsum('bhqd, bhkd -> bhqk', q, k) / jnp.sqrt(head_dim)\n",
        "\n",
        "  # Apply softmax\n",
        "  weights = softmax(scores, axis=-1)\n",
        "\n",
        "  # Apply attention weights to V for each head\n",
        "  output = jnp.einsum('bhqk, bhkd -> bhqd', weights, v)\n",
        "\n",
        "  # Concatenate head outputs\n",
        "  output = output.transpose(0, 2, 1, 3)\n",
        "  output = output.reshape(batch_size, seq_len, embed_dim)\n",
        "\n",
        "  # Final output projection\n",
        "  output = jnp.dot(output, wo)\n",
        "  return output\n",
        "\n",
        "key = jax.random.PRNGKey(3)\n",
        "seq_len = 64\n",
        "embed_dim = 256\n",
        "num_heads = 4 # Number of attention heads\n",
        "head_dim = embed_dim // num_heads # Dimensionality of each head\n",
        "\n",
        "# Simplified shapes (Batch=1, SeqLen, EmbedDim)\n",
        "input_shape = (1, seq_len, embed_dim)\n",
        "x = jax.random.normal(key, input_shape)\n",
        "\n",
        "# Dummy parameters\n",
        "keys = jax.random.split(key, 4)\n",
        "# Reshape weight matrices to incorporate head_dim\n",
        "wq = jax.random.normal(keys[0], (embed_dim, embed_dim))\n",
        "wk = jax.random.normal(keys[1], (embed_dim, embed_dim))\n",
        "wv = jax.random.normal(keys[2], (embed_dim, embed_dim))\n",
        "wo = jax.random.normal(keys[3], (embed_dim, embed_dim))\n",
        "params = (wq, wk, wv, wo)\n",
        "\n",
        "# JIT compile\n",
        "jit_attention = jax.jit(multihead_self_attention, static_argnums=(2,))\n",
        "lowered = jit_attention.lower(params, x, num_heads)\n",
        "\n",
        "# Extract the HLO text\n",
        "hlo_text = lowered.compiler_ir(dialect=\"hlo\").as_hlo_text()\n",
        "print(hlo_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE7TMEM_b_u5",
        "outputId": "000ca3b2-d5ac-459d-df2c-2834baef68ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HloModule jit_multihead_self_attention, entry_computation_layout={(f32[256,256]{1,0}, f32[256,256]{1,0}, f32[256,256]{1,0}, f32[256,256]{1,0}, f32[1,64,256]{2,1,0})->f32[1,64,256]{2,1,0}}\n",
            "\n",
            "region_0.20 {\n",
            "  Arg_0.21 = f32[] parameter(0)\n",
            "  Arg_1.22 = f32[] parameter(1)\n",
            "  ROOT maximum.23 = f32[] maximum(Arg_0.21, Arg_1.22)\n",
            "}\n",
            "\n",
            "region_1.32 {\n",
            "  Arg_0.33 = f32[] parameter(0)\n",
            "  Arg_1.34 = f32[] parameter(1)\n",
            "  ROOT add.35 = f32[] add(Arg_0.33, Arg_1.34)\n",
            "}\n",
            "\n",
            "ENTRY main.46 {\n",
            "  Arg_4.5 = f32[1,64,256]{2,1,0} parameter(4)\n",
            "  Arg_0.1 = f32[256,256]{1,0} parameter(0)\n",
            "  dot.12 = f32[1,64,256]{2,1,0} dot(Arg_4.5, Arg_0.1), lhs_contracting_dims={2}, rhs_contracting_dims={0}\n",
            "  reshape.13 = f32[1,4,64,64]{3,2,1,0} reshape(dot.12)\n",
            "  Arg_1.2 = f32[256,256]{1,0} parameter(1)\n",
            "  dot.14 = f32[1,64,256]{2,1,0} dot(Arg_4.5, Arg_1.2), lhs_contracting_dims={2}, rhs_contracting_dims={0}\n",
            "  reshape.15 = f32[1,4,64,64]{3,2,1,0} reshape(dot.14)\n",
            "  dot.18 = f32[1,4,64,64]{3,2,1,0} dot(reshape.13, reshape.15), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}\n",
            "  constant.8 = f32[] constant(8)\n",
            "  broadcast.9 = f32[1,4,64,64]{3,2,1,0} broadcast(constant.8), dimensions={}\n",
            "  divide.19 = f32[1,4,64,64]{3,2,1,0} divide(dot.18, broadcast.9)\n",
            "  constant.11 = f32[] constant(-inf)\n",
            "  reduce.24 = f32[1,4,64]{2,1,0} reduce(divide.19, constant.11), dimensions={3}, to_apply=region_0.20\n",
            "  constant.6 = f32[] constant(-inf)\n",
            "  broadcast.7 = f32[1,4,64]{2,1,0} broadcast(constant.6), dimensions={}\n",
            "  maximum.25 = f32[1,4,64]{2,1,0} maximum(reduce.24, broadcast.7)\n",
            "  reshape.26 = f32[1,4,64,1]{3,2,1,0} reshape(maximum.25)\n",
            "  broadcast.27 = f32[1,4,64,1]{3,2,1,0} broadcast(reshape.26), dimensions={0,1,2,3}\n",
            "  reshape.28 = f32[1,4,64]{2,1,0} reshape(broadcast.27)\n",
            "  broadcast.29 = f32[1,4,64,64]{3,2,1,0} broadcast(reshape.28), dimensions={0,1,2}\n",
            "  subtract.30 = f32[1,4,64,64]{3,2,1,0} subtract(divide.19, broadcast.29)\n",
            "  exponential.31 = f32[1,4,64,64]{3,2,1,0} exponential(subtract.30)\n",
            "  constant.10 = f32[] constant(0)\n",
            "  reduce.36 = f32[1,4,64]{2,1,0} reduce(exponential.31, constant.10), dimensions={3}, to_apply=region_1.32\n",
            "  reshape.37 = f32[1,4,64,1]{3,2,1,0} reshape(reduce.36)\n",
            "  broadcast.38 = f32[1,4,64,1]{3,2,1,0} broadcast(reshape.37), dimensions={0,1,2,3}\n",
            "  reshape.39 = f32[1,4,64]{2,1,0} reshape(broadcast.38)\n",
            "  broadcast.40 = f32[1,4,64,64]{3,2,1,0} broadcast(reshape.39), dimensions={0,1,2}\n",
            "  divide.41 = f32[1,4,64,64]{3,2,1,0} divide(exponential.31, broadcast.40)\n",
            "  Arg_2.3 = f32[256,256]{1,0} parameter(2)\n",
            "  dot.16 = f32[1,64,256]{2,1,0} dot(Arg_4.5, Arg_2.3), lhs_contracting_dims={2}, rhs_contracting_dims={0}\n",
            "  reshape.17 = f32[1,4,64,64]{3,2,1,0} reshape(dot.16)\n",
            "  dot.42 = f32[1,4,64,64]{3,2,1,0} dot(divide.41, reshape.17), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}\n",
            "  transpose.43 = f32[1,64,4,64]{3,1,2,0} transpose(dot.42), dimensions={0,2,1,3}\n",
            "  reshape.44 = f32[1,64,256]{2,1,0} reshape(transpose.43)\n",
            "  Arg_3.4 = f32[256,256]{1,0} parameter(3)\n",
            "  ROOT dot.45 = f32[1,64,256]{2,1,0} dot(reshape.44, Arg_3.4), lhs_contracting_dims={2}, rhs_contracting_dims={0}\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mixed Precision Forward Pass of a CNN to RELU\n",
        "    \n",
        "```\n",
        "A two-layer Convâ†’ReLU block in mixed precision:\n",
        "     1) cast inputs & weights to bfloat16\n",
        "     2) conv + bias add in bfloat16\n",
        "     3) cast back to float32 + ReLU\n",
        "     4) repeat with stride=2\n",
        "```"
      ],
      "metadata": {
        "id": "11a3LVEXlpNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import lax\n",
        "from jax.nn import relu\n",
        "\n",
        "def conv_block_mp(params, x):\n",
        "    # -- first conv --\n",
        "    x16   = lax.convert_element_type(x,    jnp.bfloat16)\n",
        "    w1_16 = lax.convert_element_type(params['w1'], jnp.bfloat16)\n",
        "    b1_16 = lax.convert_element_type(params['b1'], jnp.bfloat16)\n",
        "    y16   = lax.conv_general_dilated(\n",
        "               x16, w1_16,\n",
        "               window_strides=(1,1),\n",
        "               padding='SAME',\n",
        "               dimension_numbers=('NHWC', 'HWIO', 'NHWC')\n",
        "           ) + b1_16\n",
        "    y     = lax.convert_element_type(y16, jnp.float32)\n",
        "    y     = relu(y)\n",
        "\n",
        "    # -- second conv (downsample) --\n",
        "    y16   = lax.convert_element_type(y,    jnp.bfloat16)\n",
        "    w2_16 = lax.convert_element_type(params['w2'], jnp.bfloat16)\n",
        "    b2_16 = lax.convert_element_type(params['b2'], jnp.bfloat16)\n",
        "    z16   = lax.conv_general_dilated(\n",
        "               y16, w2_16,\n",
        "               window_strides=(2,2),\n",
        "               padding='SAME',\n",
        "               dimension_numbers=('NHWC', 'HWIO', 'NHWC')\n",
        "           ) + b2_16\n",
        "    z     = lax.convert_element_type(z16, jnp.float32)\n",
        "    return relu(z)\n",
        "\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "x0 = jax.random.normal(key, (1, 32, 32, 3))   # e.g. CIFAR-style image\n",
        "params = {\n",
        "  'w1': jax.random.normal(key, (3,3,3,16)),   # 16 filters\n",
        "  'b1': jnp.zeros((16,)),\n",
        "  'w2': jax.random.normal(key, (3,3,16,32)),  # 32 filters\n",
        "  'b2': jnp.zeros((32,))\n",
        "}\n",
        "\n",
        "# 2) jit & lower\n",
        "jit_block = jax.jit(conv_block_mp)\n",
        "lowered  = jit_block.lower(params, x0)\n",
        "\n",
        "# 3) dump HLO text\n",
        "print(lowered.compiler_ir(dialect=\"hlo\").as_hlo_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6xMiFYQlpjo",
        "outputId": "75aeaeab-841e-4fb7-91ba-a893a511da08"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HloModule jit_conv_block_mp, entry_computation_layout={(f32[16]{0}, f32[32]{0}, f32[3,3,3,16]{3,2,1,0}, f32[3,3,16,32]{3,2,1,0}, f32[1,32,32,3]{3,2,1,0})->f32[1,16,16,32]{3,2,1,0}}\n",
            "\n",
            "relu.16 {\n",
            "  Arg_0.17 = f32[1,32,32,16]{3,2,1,0} parameter(0)\n",
            "  constant.18 = f32[] constant(0)\n",
            "  broadcast.19 = f32[1,32,32,16]{3,2,1,0} broadcast(constant.18), dimensions={}\n",
            "  ROOT maximum.20 = f32[1,32,32,16]{3,2,1,0} maximum(Arg_0.17, broadcast.19)\n",
            "}\n",
            "\n",
            "relu_0.32 {\n",
            "  Arg_0.33 = f32[1,16,16,32]{3,2,1,0} parameter(0)\n",
            "  constant.34 = f32[] constant(0)\n",
            "  broadcast.35 = f32[1,16,16,32]{3,2,1,0} broadcast(constant.34), dimensions={}\n",
            "  ROOT maximum.36 = f32[1,16,16,32]{3,2,1,0} maximum(Arg_0.33, broadcast.35)\n",
            "}\n",
            "\n",
            "ENTRY main.38 {\n",
            "  Arg_4.5 = f32[1,32,32,3]{3,2,1,0} parameter(4)\n",
            "  convert.6 = bf16[1,32,32,3]{3,2,1,0} convert(Arg_4.5)\n",
            "  Arg_2.3 = f32[3,3,3,16]{3,2,1,0} parameter(2)\n",
            "  convert.7 = bf16[3,3,3,16]{3,2,1,0} convert(Arg_2.3)\n",
            "  convolution.9 = bf16[1,32,32,16]{3,2,1,0} convolution(convert.6, convert.7), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f\n",
            "  Arg_0.1 = f32[16]{0} parameter(0)\n",
            "  convert.8 = bf16[16]{0} convert(Arg_0.1)\n",
            "  reshape.10 = bf16[1,1,1,16]{3,2,1,0} reshape(convert.8)\n",
            "  broadcast.11 = bf16[1,1,1,16]{3,2,1,0} broadcast(reshape.10), dimensions={0,1,2,3}\n",
            "  reshape.12 = bf16[1,16]{1,0} reshape(broadcast.11)\n",
            "  broadcast.13 = bf16[1,32,32,16]{3,2,1,0} broadcast(reshape.12), dimensions={0,3}\n",
            "  add.14 = bf16[1,32,32,16]{3,2,1,0} add(convolution.9, broadcast.13)\n",
            "  convert.15 = f32[1,32,32,16]{3,2,1,0} convert(add.14)\n",
            "  call.21 = f32[1,32,32,16]{3,2,1,0} call(convert.15), to_apply=relu.16\n",
            "  convert.22 = bf16[1,32,32,16]{3,2,1,0} convert(call.21)\n",
            "  Arg_3.4 = f32[3,3,16,32]{3,2,1,0} parameter(3)\n",
            "  convert.23 = bf16[3,3,16,32]{3,2,1,0} convert(Arg_3.4)\n",
            "  convolution.25 = bf16[1,16,16,32]{3,2,1,0} convolution(convert.22, convert.23), window={size=3x3 stride=2x2 pad=0_1x0_1}, dim_labels=b01f_01io->b01f\n",
            "  Arg_1.2 = f32[32]{0} parameter(1)\n",
            "  convert.24 = bf16[32]{0} convert(Arg_1.2)\n",
            "  reshape.26 = bf16[1,1,1,32]{3,2,1,0} reshape(convert.24)\n",
            "  broadcast.27 = bf16[1,1,1,32]{3,2,1,0} broadcast(reshape.26), dimensions={0,1,2,3}\n",
            "  reshape.28 = bf16[1,32]{1,0} reshape(broadcast.27)\n",
            "  broadcast.29 = bf16[1,16,16,32]{3,2,1,0} broadcast(reshape.28), dimensions={0,3}\n",
            "  add.30 = bf16[1,16,16,32]{3,2,1,0} add(convolution.25, broadcast.29)\n",
            "  convert.31 = f32[1,16,16,32]{3,2,1,0} convert(add.30)\n",
            "  ROOT call.37 = f32[1,16,16,32]{3,2,1,0} call(convert.31), to_apply=relu_0.32\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AvF8dZ-1pbL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "from functools import partial\n",
        "\n",
        "# 1) Set up a simple optimizer (e.g. SGD)\n",
        "learning_rate = 1e-2\n",
        "opt = optax.sgd(learning_rate)\n",
        "\n",
        "# 2) Define the pmapped train step\n",
        "@partial(jax.pmap, axis_name=\"batch\")\n",
        "def train_step(params, opt_state, batch):\n",
        "    # Forward+loss\n",
        "    def loss_fn(p, x, y):\n",
        "        logits = x @ p['w'] + p['b']\n",
        "        return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits, y))\n",
        "    (loss, grads) = jax.value_and_grad(loss_fn)(params, batch['x'], batch['y'])\n",
        "\n",
        "    # All-reduce the gradients across devices\n",
        "    grads = jax.lax.pmean(grads, axis_name=\"batch\")\n",
        "\n",
        "    # Optimizer update\n",
        "    updates, new_opt_state = opt.update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "\n",
        "    return new_params, new_opt_state, loss\n",
        "\n",
        "# 3) Dummy data setup for N devices\n",
        "num_devices = jax.local_device_count()\n",
        "per_device_batch = 8\n",
        "feature_dim = 16\n",
        "num_classes = 10\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "# Create one key per device\n",
        "keys = jax.random.split(key, num_devices)\n",
        "# Dummy inputs: shape (num_devices, per_device_batch, feature_dim)\n",
        "x = jnp.stack([jax.random.normal(k, (per_device_batch, feature_dim)) for k in keys])\n",
        "y = jnp.stack([jax.random.randint(k, (per_device_batch,), 0, num_classes) for k in keys])\n",
        "batch = {'x': x, 'y': y}\n",
        "\n",
        "# 4) Initialize & shard (replicate) parameters and optimizer state\n",
        "init_params = {\n",
        "    'w': jax.random.normal(key, (feature_dim, num_classes)),\n",
        "    'b': jnp.zeros((num_classes,))\n",
        "}\n",
        "# replicate across devices\n",
        "params = jax.device_put_replicated(init_params, jax.local_devices())\n",
        "opt_state = jax.device_put_replicated(opt.init(init_params), jax.local_devices())\n",
        "\n",
        "# 5) Lower & dump the HLO\n",
        "lowered = train_step.lower(params, opt_state, batch)\n",
        "hlo_text = lowered.compiler_ir(dialect=\"hlo\").as_hlo_text()\n",
        "print(hlo_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAS7zLq6paum",
        "outputId": "7ee605e9-b7a8-4546-9a31-0c9705495b2c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HloModule pmap_train_step, entry_computation_layout={(f32[1,10]{1,0}, f32[1,16,10]{2,1,0}, f32[1,8,16]{2,1,0}, s32[1,8]{1,0})->(f32[1,10]{1,0}, f32[1,16,10]{2,1,0}, f32[1]{0})}\n",
            "\n",
            "region_0.31 {\n",
            "  Arg_0.32 = f32[] parameter(0)\n",
            "  Arg_1.33 = f32[] parameter(1)\n",
            "  ROOT maximum.34 = f32[] maximum(Arg_0.32, Arg_1.33)\n",
            "}\n",
            "\n",
            "region_1.43 {\n",
            "  Arg_0.44 = pred[] parameter(0)\n",
            "  Arg_1.45 = pred[] parameter(1)\n",
            "  ROOT and.46 = pred[] and(Arg_0.44, Arg_1.45)\n",
            "}\n",
            "\n",
            "take_along_axis.47 {\n",
            "  Arg_1.49 = s32[8,1]{1,0} parameter(1)\n",
            "  constant.59 = s32[] constant(0)\n",
            "  broadcast.60 = s32[8,1]{1,0} broadcast(constant.59), dimensions={}\n",
            "  compare.61 = pred[8,1]{1,0} compare(Arg_1.49, broadcast.60), direction=LT\n",
            "  constant.56 = s32[] constant(10)\n",
            "  broadcast.57 = s32[8,1]{1,0} broadcast(constant.56), dimensions={}\n",
            "  add.62 = s32[8,1]{1,0} add(Arg_1.49, broadcast.57)\n",
            "  select.63 = s32[8,1]{1,0} select(compare.61, add.62, Arg_1.49)\n",
            "  reshape.64 = s32[8,1,1]{2,1,0} reshape(select.63)\n",
            "  constant.54 = s32[] constant(0)\n",
            "  broadcast.55 = s32[8,1,1]{2,1,0} broadcast(constant.54), dimensions={}\n",
            "  compare.65 = pred[8,1,1]{2,1,0} compare(reshape.64, broadcast.55), direction=GE\n",
            "  constant.52 = s32[] constant(9)\n",
            "  broadcast.53 = s32[8,1,1]{2,1,0} broadcast(constant.52), dimensions={}\n",
            "  compare.66 = pred[8,1,1]{2,1,0} compare(reshape.64, broadcast.53), direction=LE\n",
            "  and.67 = pred[8,1,1]{2,1,0} and(compare.65, compare.66)\n",
            "  constant.58 = pred[] constant(true)\n",
            "  reduce.68 = pred[8,1]{1,0} reduce(and.67, constant.58), dimensions={2}, to_apply=region_1.43\n",
            "  Arg_0.48 = f32[8,10]{1,0} parameter(0)\n",
            "  gather.69 = f32[8,1]{1,0} gather(Arg_0.48, reshape.64), offset_dims={}, collapsed_slice_dims={1}, start_index_map={1}, operand_batching_dims={0}, start_indices_batching_dims={0}, index_vector_dim=2, slice_sizes={1,1}\n",
            "  constant.50 = f32[] constant(nan)\n",
            "  broadcast.51 = f32[8,1]{1,0} broadcast(constant.50), dimensions={}\n",
            "  select.70 = f32[8,1]{1,0} select(reduce.68, gather.69, broadcast.51)\n",
            "  ROOT tuple.71 = (f32[8,1]{1,0}, s32[8,1,1]{2,1,0}) tuple(select.70, reshape.64)\n",
            "}\n",
            "\n",
            "_where.75 {\n",
            "  Arg_0.76 = pred[] parameter(0)\n",
            "  Arg_1.77 = s32[] parameter(1)\n",
            "  Arg_2.78 = s32[] parameter(2)\n",
            "  ROOT select.79 = s32[] select(Arg_0.76, Arg_1.77, Arg_2.78)\n",
            "}\n",
            "\n",
            "region_2.80 {\n",
            "  Arg_0.81 = pred[] parameter(0)\n",
            "  Arg_1.82 = pred[] parameter(1)\n",
            "  ROOT and.83 = pred[] and(Arg_0.81, Arg_1.82)\n",
            "}\n",
            "\n",
            "_take.84 {\n",
            "  Arg_1.86 = s32[] parameter(1)\n",
            "  constant.92 = s32[] constant(0)\n",
            "  compare.93 = pred[] compare(Arg_1.86, constant.92), direction=LT\n",
            "  constant.91 = s32[] constant(1)\n",
            "  add.94 = s32[] add(Arg_1.86, constant.91)\n",
            "  call.95 = s32[] call(compare.93, add.94, Arg_1.86), to_apply=_where.75\n",
            "  reshape.96 = s32[1]{0} reshape(call.95)\n",
            "  constant.90 = s32[1]{0} constant({0})\n",
            "  compare.97 = pred[1]{0} compare(reshape.96, constant.90), direction=GE\n",
            "  compare.98 = pred[1]{0} compare(reshape.96, constant.90), direction=LE\n",
            "  and.99 = pred[1]{0} and(compare.97, compare.98)\n",
            "  constant.89 = pred[] constant(true)\n",
            "  reduce.100 = pred[] reduce(and.99, constant.89), dimensions={0}, to_apply=region_2.80\n",
            "  broadcast.102 = pred[8]{0} broadcast(reduce.100), dimensions={}\n",
            "  Arg_0.85 = f32[8,1]{1,0} parameter(0)\n",
            "  gather.101 = f32[8]{0} gather(Arg_0.85, reshape.96), offset_dims={0}, collapsed_slice_dims={1}, start_index_map={1}, index_vector_dim=0, slice_sizes={8,1}\n",
            "  constant.87 = f32[] constant(nan)\n",
            "  broadcast.88 = f32[8]{0} broadcast(constant.87), dimensions={}\n",
            "  select.103 = f32[8]{0} select(broadcast.102, gather.101, broadcast.88)\n",
            "  ROOT tuple.104 = (f32[8]{0}, s32[1]{0}) tuple(select.103, reshape.96)\n",
            "}\n",
            "\n",
            "region_3.109 {\n",
            "  Arg_0.110 = f32[] parameter(0)\n",
            "  Arg_1.111 = f32[] parameter(1)\n",
            "  ROOT add.112 = f32[] add(Arg_0.110, Arg_1.111)\n",
            "}\n",
            "\n",
            "region_4.116 {\n",
            "  Arg_0.117 = f32[] parameter(0)\n",
            "  Arg_1.118 = f32[] parameter(1)\n",
            "  ROOT add.119 = f32[] add(Arg_0.117, Arg_1.118)\n",
            "}\n",
            "\n",
            "region_5.122 {\n",
            "  Arg_0.123 = f32[] parameter(0)\n",
            "  Arg_1.124 = f32[] parameter(1)\n",
            "  ROOT add.125 = f32[] add(Arg_0.123, Arg_1.124)\n",
            "}\n",
            "\n",
            "_take_0.126 {\n",
            "  constant.129 = f32[] constant(0)\n",
            "  broadcast.130 = f32[8,1]{1,0} broadcast(constant.129), dimensions={}\n",
            "  Arg_0.127 = s32[1]{0} parameter(0)\n",
            "  Arg_1.128 = f32[8]{0} parameter(1)\n",
            "  ROOT scatter.131 = f32[8,1]{1,0} scatter(broadcast.130, Arg_0.127, Arg_1.128), update_window_dims={0}, inserted_window_dims={1}, scatter_dims_to_operand_dims={1}, index_vector_dim=0, to_apply=region_5.122\n",
            "}\n",
            "\n",
            "region_6.133 {\n",
            "  Arg_0.134 = f32[] parameter(0)\n",
            "  Arg_1.135 = f32[] parameter(1)\n",
            "  ROOT add.136 = f32[] add(Arg_0.134, Arg_1.135)\n",
            "}\n",
            "\n",
            "take_along_axis_1.137 {\n",
            "  constant.140 = f32[] constant(0)\n",
            "  broadcast.141 = f32[8,10]{1,0} broadcast(constant.140), dimensions={}\n",
            "  Arg_0.138 = s32[8,1,1]{2,1,0} parameter(0)\n",
            "  Arg_1.139 = f32[8,1]{1,0} parameter(1)\n",
            "  ROOT scatter.142 = f32[8,10]{1,0} scatter(broadcast.141, Arg_0.138, Arg_1.139), update_window_dims={}, inserted_window_dims={1}, scatter_dims_to_operand_dims={1}, input_batching_dims={0}, scatter_indices_batching_dims={0}, index_vector_dim=2, to_apply=region_6.133\n",
            "}\n",
            "\n",
            "region_7.148 {\n",
            "  Arg_0.149 = f32[] parameter(0)\n",
            "  Arg_1.150 = f32[] parameter(1)\n",
            "  ROOT add.151 = f32[] add(Arg_0.149, Arg_1.150)\n",
            "}\n",
            "\n",
            "region_8.154 {\n",
            "  Arg_0.155 = f32[] parameter(0)\n",
            "  Arg_1.156 = f32[] parameter(1)\n",
            "  ROOT add.157 = f32[] add(Arg_0.155, Arg_1.156)\n",
            "}\n",
            "\n",
            "region_9.161 {\n",
            "  Arg_0.162 = f32[] parameter(0)\n",
            "  Arg_1.163 = f32[] parameter(1)\n",
            "  ROOT add.164 = f32[] add(Arg_0.162, Arg_1.163)\n",
            "}\n",
            "\n",
            "region_10.166 {\n",
            "  Arg_0.167 = f32[] parameter(0)\n",
            "  Arg_1.168 = f32[] parameter(1)\n",
            "  ROOT add.169 = f32[] add(Arg_0.167, Arg_1.168)\n",
            "}\n",
            "\n",
            "ENTRY main.181 {\n",
            "  Arg_0.1 = f32[1,10]{1,0} parameter(0)\n",
            "  reshape.23 = f32[10]{0} reshape(Arg_0.1)\n",
            "  Arg_2.3 = f32[1,8,16]{2,1,0} parameter(2)\n",
            "  reshape.25 = f32[8,16]{1,0} reshape(Arg_2.3)\n",
            "  Arg_1.2 = f32[1,16,10]{2,1,0} parameter(1)\n",
            "  reshape.24 = f32[16,10]{1,0} reshape(Arg_1.2)\n",
            "  dot.26 = f32[8,10]{1,0} dot(reshape.25, reshape.24), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n",
            "  broadcast.27 = f32[1,10]{1,0} broadcast(Arg_0.1), dimensions={0,1}\n",
            "  reshape.28 = f32[10]{0} reshape(broadcast.27)\n",
            "  broadcast.29 = f32[8,10]{1,0} broadcast(reshape.28), dimensions={1}\n",
            "  add.30 = f32[8,10]{1,0} add(dot.26, broadcast.29)\n",
            "  constant.22 = f32[] constant(-inf)\n",
            "  reduce.35 = f32[8]{0} reduce(add.30, constant.22), dimensions={1}, to_apply=region_0.31\n",
            "  constant.17 = f32[] constant(-inf)\n",
            "  broadcast.18 = f32[8]{0} broadcast(constant.17), dimensions={}\n",
            "  maximum.36 = f32[8]{0} maximum(reduce.35, broadcast.18)\n",
            "  reshape.37 = f32[8,1]{1,0} reshape(maximum.36)\n",
            "  broadcast.38 = f32[8,1]{1,0} broadcast(reshape.37), dimensions={0,1}\n",
            "  reshape.39 = f32[8]{0} reshape(broadcast.38)\n",
            "  broadcast.40 = f32[8,10]{1,0} broadcast(reshape.39), dimensions={0}\n",
            "  subtract.41 = f32[8,10]{1,0} subtract(add.30, broadcast.40)\n",
            "  Arg_3.4 = s32[1,8]{1,0} parameter(3)\n",
            "  reshape.42 = s32[8,1]{1,0} reshape(Arg_3.4)\n",
            "  call.72 = (f32[8,1]{1,0}, s32[8,1,1]{2,1,0}) call(subtract.41, reshape.42), to_apply=take_along_axis.47\n",
            "  get-tuple-element.74 = s32[8,1,1]{2,1,0} get-tuple-element(call.72), index=1\n",
            "  get-tuple-element.73 = f32[8,1]{1,0} get-tuple-element(call.72), index=0\n",
            "  constant.20 = s32[] constant(0)\n",
            "  call.105 = (f32[8]{0}, s32[1]{0}) call(get-tuple-element.73, constant.20), to_apply=_take.84\n",
            "  get-tuple-element.107 = s32[1]{0} get-tuple-element(call.105), index=1\n",
            "  constant.13 = f32[] constant(-0.125)\n",
            "  broadcast.14 = f32[8]{0} broadcast(constant.13), dimensions={}\n",
            "  call.132 = f32[8,1]{1,0} call(get-tuple-element.107, broadcast.14), to_apply=_take_0.126\n",
            "  call.143 = f32[8,10]{1,0} call(get-tuple-element.74, call.132), to_apply=take_along_axis_1.137\n",
            "  constant.15 = f32[] constant(0.125)\n",
            "  broadcast.16 = f32[8]{0} broadcast(constant.15), dimensions={}\n",
            "  exponential.108 = f32[8,10]{1,0} exponential(subtract.41)\n",
            "  constant.21 = f32[] constant(0)\n",
            "  reduce.113 = f32[8]{0} reduce(exponential.108, constant.21), dimensions={1}, to_apply=region_3.109\n",
            "  divide.144 = f32[8]{0} divide(broadcast.16, reduce.113)\n",
            "  broadcast.145 = f32[8,10]{1,0} broadcast(divide.144), dimensions={0}\n",
            "  multiply.146 = f32[8,10]{1,0} multiply(broadcast.145, exponential.108)\n",
            "  add.147 = f32[8,10]{1,0} add(call.143, multiply.146)\n",
            "  reduce.152 = f32[10]{0} reduce(add.147, constant.21), dimensions={0}, to_apply=region_7.148\n",
            "  reshape.153 = f32[1,10]{1,0} reshape(reduce.152)\n",
            "  reduce.158 = f32[10]{0} reduce(reshape.153, constant.21), dimensions={0}, to_apply=region_8.154\n",
            "  all-reduce.165 = f32[10]{0} all-reduce(reduce.158), replica_groups={{0}}, to_apply=region_9.161\n",
            "  constant.11 = f32[] constant(1)\n",
            "  broadcast.12 = f32[10]{0} broadcast(constant.11), dimensions={}\n",
            "  divide.171 = f32[10]{0} divide(all-reduce.165, broadcast.12)\n",
            "  constant.7 = f32[] constant(-0.01)\n",
            "  broadcast.8 = f32[10]{0} broadcast(constant.7), dimensions={}\n",
            "  multiply.173 = f32[10]{0} multiply(divide.171, broadcast.8)\n",
            "  add.175 = f32[10]{0} add(reshape.23, multiply.173)\n",
            "  reshape.177 = f32[1,10]{1,0} reshape(add.175)\n",
            "  dot.159 = f32[10,16]{1,0} dot(add.147, reshape.25), lhs_contracting_dims={0}, rhs_contracting_dims={0}\n",
            "  transpose.160 = f32[16,10]{0,1} transpose(dot.159), dimensions={1,0}\n",
            "  all-reduce.170 = f32[16,10]{0,1} all-reduce(transpose.160), replica_groups={{0}}, to_apply=region_10.166\n",
            "  constant.9 = f32[] constant(1)\n",
            "  broadcast.10 = f32[16,10]{1,0} broadcast(constant.9), dimensions={}\n",
            "  divide.172 = f32[16,10]{0,1} divide(all-reduce.170, broadcast.10)\n",
            "  constant.5 = f32[] constant(-0.01)\n",
            "  broadcast.6 = f32[16,10]{1,0} broadcast(constant.5), dimensions={}\n",
            "  multiply.174 = f32[16,10]{0,1} multiply(divide.172, broadcast.6)\n",
            "  add.176 = f32[16,10]{1,0} add(reshape.24, multiply.174)\n",
            "  reshape.178 = f32[1,16,10]{2,1,0} reshape(add.176)\n",
            "  log.114 = f32[8]{0} log(reduce.113)\n",
            "  get-tuple-element.106 = f32[8]{0} get-tuple-element(call.105), index=0\n",
            "  subtract.115 = f32[8]{0} subtract(log.114, get-tuple-element.106)\n",
            "  reduce.120 = f32[] reduce(subtract.115, constant.21), dimensions={0}, to_apply=region_4.116\n",
            "  constant.19 = f32[] constant(8)\n",
            "  divide.121 = f32[] divide(reduce.120, constant.19)\n",
            "  reshape.179 = f32[1]{0} reshape(divide.121)\n",
            "  ROOT tuple.180 = (f32[1,10]{1,0}, f32[1,16,10]{2,1,0}, f32[1]{0}) tuple(reshape.177, reshape.178, reshape.179)\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}